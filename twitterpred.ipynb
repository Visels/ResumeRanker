{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Visels/ResumeRanker/blob/main/twitterpred.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install Unidecode"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KNS-DmmK9X5d",
        "outputId": "f1448b10-2623-4103-ada3-3bd3ad43967a"
      },
      "id": "KNS-DmmK9X5d",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting Unidecode\n",
            "  Downloading Unidecode-1.3.6-py3-none-any.whl (235 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.9/235.9 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: Unidecode\n",
            "Successfully installed Unidecode-1.3.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eb55b009-7c00-427c-9f4f-58e61010b434",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eb55b009-7c00-427c-9f4f-58e61010b434",
        "outputId": "f88ec16e-1d23-470b-a608-3d58516bccfc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:1380: UserWarning: Upper case characters found in vocabulary while 'lowercase' is True. These entries will not be matched with any documents\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(265610, 302)\n",
            "(66403, 302)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:1380: UserWarning: Upper case characters found in vocabulary while 'lowercase' is True. These entries will not be matched with any documents\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(342280, 302)\n",
            "(85570, 302)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:1380: UserWarning: Upper case characters found in vocabulary while 'lowercase' is True. These entries will not be matched with any documents\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(256000, 302)\n",
            "(64000, 302)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:1380: UserWarning: Upper case characters found in vocabulary while 'lowercase' is True. These entries will not be matched with any documents\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(188541, 302)\n",
            "(47135, 302)\n"
          ]
        }
      ],
      "source": [
        "import csv\n",
        "import array\n",
        "import pandas\n",
        "import pickle\n",
        "import os\n",
        "import sys\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn import svm\n",
        "csvFile=open('newfrequency300.csv', 'rt')\n",
        "csvReader=csv.reader(csvFile)\n",
        "mydict={row[1]: int(row[0]) for row in csvReader}\n",
        "\n",
        "y=[]\n",
        "with open ('PJFinaltest.csv', 'rt') as f:\n",
        "\treader=csv.reader(f)\n",
        "\tcorpus=[rows[0] for rows in reader]\n",
        "\t\n",
        "\n",
        "with open ('PJFinaltest.csv', 'rt') as f:\n",
        "\tcsvReader1=csv.reader(f)\n",
        "\tfor rows in csvReader1:\n",
        "\t\ty.append([int(rows[1])])\n",
        "vectorizer=TfidfVectorizer(vocabulary=mydict,min_df=1)\n",
        "x=vectorizer.fit_transform(corpus).toarray()\n",
        "result=np.append(x,y,axis=1)\n",
        "X=pandas.DataFrame(result)\n",
        "model=GaussianNB()\n",
        "train = X.sample(frac=0.8, random_state=1)\n",
        "test=X.drop(train.index)\n",
        "y_train=train[301]\n",
        "y_test=test[301]\n",
        "print(train.shape)\n",
        "print(test.shape)\n",
        "xtrain=train.drop(301,axis=1)\n",
        "xtest=test.drop(301,axis=1)\n",
        "model.fit(xtrain,y_train)\n",
        "pickle.dump(model, open('BNPJFinal.sav', 'wb'))\n",
        "del result\n",
        "\n",
        "y=[]\n",
        "with open ('IEFinaltest.csv', 'rt') as f:\n",
        "\treader=csv.reader(f)\n",
        "\tcorpus=[rows[0] for rows in reader]\n",
        "\n",
        "with open ('IEFinaltest.csv', 'rt') as f:\n",
        "\tcsvReader1=csv.reader(f)\n",
        "\tfor rows in csvReader1:\n",
        "\t\ty.append([int(rows[1])])\n",
        "vectorizer=TfidfVectorizer(vocabulary=mydict,min_df=1)\n",
        "x=vectorizer.fit_transform(corpus).toarray()\n",
        "result=np.append(x,y,axis=1)\n",
        "X=pandas.DataFrame(result)\n",
        "model=GaussianNB()\n",
        "train = X.sample(frac=0.8, random_state=1)\n",
        "test=X.drop(train.index)\n",
        "y_train=train[301]\n",
        "y_test=test[301]\n",
        "print(train.shape)\n",
        "print(test.shape)\n",
        "xtrain=train.drop(301,axis=1)\n",
        "xtest=test.drop(301,axis=1)\n",
        "model.fit(xtrain,y_train)\n",
        "pickle.dump(model, open('BNIEFinal.sav', 'wb'))\n",
        "del result\n",
        "\n",
        "y=[]\n",
        "with open ('TFFinaltest.csv', 'rt') as f:\n",
        "\treader=csv.reader(f)\n",
        "\tcorpus=[rows[0] for rows in reader]\n",
        "\n",
        "with open ('TFFinaltest.csv', 'rt') as f:\n",
        "\tcsvReader1=csv.reader(f)\n",
        "\tfor rows in csvReader1:\n",
        "\t\ty.append([int(rows[1])])\n",
        "vectorizer=TfidfVectorizer(vocabulary=mydict,min_df=1)\n",
        "x=vectorizer.fit_transform(corpus).toarray()\n",
        "result=np.append(x,y,axis=1)\n",
        "X=pandas.DataFrame(result)\n",
        "model=GaussianNB()\n",
        "train = X.sample(frac=0.8, random_state=1)\n",
        "test=X.drop(train.index)\n",
        "y_train=train[301]\n",
        "y_test=test[301]\n",
        "print(train.shape)\n",
        "print(test.shape)\n",
        "xtrain=train.drop(301,axis=1)\n",
        "xtest=test.drop(301,axis=1)\n",
        "model.fit(xtrain,y_train)\n",
        "pickle.dump(model, open('BNTFFinal.sav', 'wb'))\n",
        "del result\n",
        "\n",
        "y=[]\n",
        "with open ('SNFinaltest.csv', 'rt') as f:\n",
        "\treader=csv.reader(f)\n",
        "\tcorpus=[rows[0] for rows in reader]\n",
        "\n",
        "with open ('SNFinaltest.csv', 'rt') as f:\n",
        "\tcsvReader1=csv.reader(f)\n",
        "\tfor rows in csvReader1:\n",
        "\t\ty.append([int(rows[1])])\n",
        "vectorizer=TfidfVectorizer(vocabulary=mydict,min_df=1)\n",
        "x=vectorizer.fit_transform(corpus).toarray()\n",
        "result=np.append(x,y,axis=1)\n",
        "X=pandas.DataFrame(result)\n",
        "model=GaussianNB()\n",
        "train = X.sample(frac=0.8, random_state=1)\n",
        "test=X.drop(train.index)\n",
        "y_train=train[301]\n",
        "y_test=test[301]\n",
        "print(train.shape)\n",
        "print(test.shape)\n",
        "xtrain=train.drop(301,axis=1)\n",
        "xtest=test.drop(301,axis=1)\n",
        "model.fit(xtrain,y_train)\n",
        "pickle.dump(model, open('BNSNFinal.sav', 'wb'))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install snscrape"
      ],
      "metadata": {
        "id": "5ocZcDKMNi8j",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c6a6a764-5673-47a2-ab24-08e8e2843245"
      },
      "id": "5ocZcDKMNi8j",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting snscrape\n",
            "  Downloading snscrape-0.6.2.20230320-py3-none-any.whl (71 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.8/71.8 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from snscrape) (4.9.2)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.10/dist-packages (from snscrape) (2.27.1)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from snscrape) (4.11.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from snscrape) (3.12.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->snscrape) (2.4.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->snscrape) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->snscrape) (1.26.15)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->snscrape) (3.4)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->snscrape) (2.0.12)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->snscrape) (1.7.1)\n",
            "Installing collected packages: snscrape\n",
            "Successfully installed snscrape-0.6.2.20230320\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fe20752f-be29-421e-975e-3946dcd70565",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fe20752f-be29-421e-975e-3946dcd70565",
        "outputId": "76c340e6-5372-47f0-ed22-b5d0b1cd3bec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Please Enter Twitter Account handle: elonmusk\n",
            "[(1.0, 30)]\n",
            "[(0.0, 37)]\n",
            "[(0.0, 29)]\n",
            "[(1.0, 33)]\n",
            "INFP\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:1380: UserWarning: Upper case characters found in vocabulary while 'lowercase' is True. These entries will not be matched with any documents\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import *\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "import tweepy\n",
        "import sys\n",
        "import os\n",
        "import nltk \n",
        "import re\n",
        "import numpy as np\n",
        "import string\n",
        "from unidecode import unidecode\n",
        "import csv\n",
        "from itertools import islice\n",
        "import pandas as pd\n",
        "import pickle\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from collections import Counter\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "import pandas as pd\n",
        "from tqdm.notebook import tqdm\n",
        "import snscrape.modules.twitter as sntwitter\n",
        "\n",
        "ckey='BQDFBbn6oDUGiXnm0G2Vo2GMF'\n",
        "csecret='pB7fJ4OAn78D8tw3DsiOZDS2LDEh8a8inzMkLN233yNg8HJ9e4'\n",
        "atoken='1017706351-hIZTuZ3cvNRrPd9adQkRbzNUj1mM2vprBFMKhAm'\n",
        "asecret='6YqkmqcMMCqr2rtkOlaJTy0LTL26J858dm38OTKD7d5WN'\n",
        "\n",
        "atoken = '1017706351-hIZTuZ3cvNRrPd9adQkRbzNUj1mM2vprBFMKhAm'\n",
        "asecret='6YqkmqcMMCqr2rtkOlaJTy0LTL26J858dm38OTKD7d5WN'\n",
        "ckey = 'ET1hPM4buF85jkVFHp0kABo2M'\n",
        "csecret = 'hdDSPzQCZC7lFWIuF9vReRZDYNAra2Y8m9PSDMGYbwusFoYJcL'\n",
        "BEARER_TOKEN = 'AAAAAAAAAAAAAAAAAAAAAECJnAEAAAAAcxSdTUxNsw%2B8yIfL9A%2F%2BF3pG5tY%3Dq3QPqG0Mdfl464IRkML8bOHIuntVVvJkG9mcetE6VQ99XKGakR' \n",
        "auth=tweepy.OAuthHandler(ckey, csecret)\n",
        "auth.set_access_token(atoken, asecret)\n",
        "api=tweepy.API(auth)\n",
        "\n",
        "emoticons_str = r\"\"\"\n",
        "    (?:\n",
        "        [:=;] # Eyes\n",
        "        [oO\\-]? # Nose (optional)\n",
        "        [D\\)\\]\\(\\]/\\\\OpP] # Mouth\n",
        "    )\"\"\"\n",
        "\n",
        "emoji_pattern = re.compile(\"[\"\n",
        "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "                           \"]+\", flags=re.UNICODE)\n",
        "\n",
        "\n",
        "regex_str = [\n",
        "    emoticons_str,\n",
        "    r'<[^>]+>',  # HTML tags\n",
        "    r'(?:@[\\w_]+)',  # @-mentions\n",
        "    r\"(?:\\#+[\\w_]+[\\w\\'_\\-]*[\\w_]+)\",  # hash-tags\n",
        "    r'http[s]?://(?:[a-z]|[0-9]|[$-_@.&amp;+]|[!*\\(\\),]|(?:%[0-9a-f][0-9a-f]))+',  # URLs\n",
        "\n",
        "    r'(?:(?:\\d+,?)+(?:\\.?\\d+)?)',  # numbers\n",
        "    r\"(?:[a-z][a-z'\\-_]+[a-z])\",  # words with - and '\n",
        "    r'(?:[\\w_]+)',  # other words\n",
        "    r'(?:\\S)'  # anything else\n",
        "]\n",
        "\n",
        "tokens_re = re.compile(r'(' + '|'.join(regex_str) + ')', re.VERBOSE | re.IGNORECASE)\n",
        "emoticon_re = re.compile(r'^' + emoticons_str + '$', re.VERBOSE | re.IGNORECASE)\n",
        "\n",
        "\n",
        "def tokenize(s):\n",
        "    return tokens_re.findall(s)\n",
        "\n",
        "\n",
        "def preprocess(s, lowercase=False):\n",
        "    tokens = tokenize(s)\n",
        "    if lowercase:\n",
        "        tokens = [token if emoticon_re.search(token) else token.lower() for token in tokens]\n",
        "    return tokens\n",
        "\n",
        "def preproc(s):\n",
        "\t#s=emoji_pattern.sub(r'', s) # no emoji\n",
        "\ts= unidecode(s)\n",
        "\tPOSTagger=preprocess(s)\n",
        "\t#print(POSTagger)\n",
        "\n",
        "\ttweet=' '.join(POSTagger)\n",
        "\tstop_words = set(stopwords.words('english'))\n",
        "\tword_tokens = word_tokenize(tweet)\n",
        "\t#filtered_sentence = [w for w in word_tokens if not w in stop_words]\n",
        "\tfiltered_sentence = []\n",
        "\tfor w in POSTagger:\n",
        "\t    if w not in stop_words:\n",
        "\t        filtered_sentence.append(w)\n",
        "\t#print(word_tokens)\n",
        "\t#print(filtered_sentence)\n",
        "\tstemmed_sentence=[]\n",
        "\tstemmer2 = SnowballStemmer(\"english\", ignore_stopwords=True)\n",
        "\tfor w in filtered_sentence:\n",
        "\t\tstemmed_sentence.append(stemmer2.stem(w))\n",
        "\t#print(stemmed_sentence)\n",
        "\n",
        "\ttemp = ' '.join(c for c in stemmed_sentence if c not in string.punctuation) \n",
        "\tpreProcessed=temp.split(\" \")\n",
        "\tfinal=[]\n",
        "\tfor i in preProcessed:\n",
        "\t\tif i not in final:\n",
        "\t\t\tif i.isdigit():\n",
        "\t\t\t\tpass\n",
        "\t\t\telse:\n",
        "\t\t\t\tif 'http' not in i:\n",
        "\t\t\t\t\tfinal.append(i)\n",
        "\ttemp1=' '.join(c for c in final)\n",
        "\t#print(preProcessed)\n",
        "\treturn temp1\n",
        "\n",
        "def getTweets(username):\n",
        "\t# csvFile = open('user.csv', 'a', newline='')\n",
        "\t# csvWriter = csv.writer(csvFile)\n",
        "\ttry:\n",
        "\t\tscrapper = sntwitter.TwitterProfileScraper(user=username)\n",
        "\t\ttweets = []\n",
        "\t\tfor i,tweet in enumerate(scrapper.get_items()):\n",
        "\t\t\tdata = [tweet.rawContent]\n",
        "\t\t\ttweets.append(data)\n",
        "\n",
        "\t\t\tif i>50:\n",
        "\t\t\t\tbreak\n",
        "\t\tdf = pd.DataFrame(tweets)\n",
        "\t\tdf.to_csv('user.csv',index=False)\n",
        "\n",
        "\texcept AttributeError:\n",
        "\t# except tweepy.TweepError:\n",
        "\t\tprint(\"Failed to run the command on that user, Skipping...\")\n",
        "\tcsvFile.close()\n",
        "\n",
        "\n",
        "\n",
        "username=input(\"Please Enter Twitter Account handle: \")\n",
        "getTweets(username)\n",
        "with open('user.csv','rt') as f:\n",
        "\tcsvReader=csv.reader(f)\n",
        "\ttweetList=[rows[0] for rows in csvReader]\n",
        "with open('newfrequency300.csv','rt') as f:\n",
        "\tcsvReader=csv.reader(f)\n",
        "\tmydict={rows[1]: int(rows[0]) for rows in csvReader}\n",
        "\n",
        "vectorizer=TfidfVectorizer(vocabulary=mydict,min_df=1)\n",
        "x=vectorizer.fit_transform(tweetList).toarray()\n",
        "df=pd.DataFrame(x)\n",
        "\n",
        "\n",
        "model_IE = pickle.load(open(\"BNIEFinal.sav\", 'rb'))\n",
        "model_SN = pickle.load(open(\"BNSNFinal.sav\", 'rb'))\n",
        "model_TF = pickle.load(open('BNTFFinal.sav', 'rb'))\n",
        "model_PJ = pickle.load(open('BNPJFinal.sav', 'rb'))\n",
        "\n",
        "answer=[]\n",
        "IE=model_IE.predict(df)\n",
        "SN=model_SN.predict(df)\n",
        "TF=model_TF.predict(df)\n",
        "PJ=model_PJ.predict(df)\n",
        "\n",
        "\n",
        "b = Counter(IE)\n",
        "value=b.most_common(1)\n",
        "print(value)\n",
        "if value[0][0] == 1.0:\n",
        "\tanswer.append(\"I\")\n",
        "else:\n",
        "\tanswer.append(\"E\")\n",
        "\n",
        "b = Counter(SN)\n",
        "value=b.most_common(1)\n",
        "print(value)\n",
        "if value[0][0] == 1.0:\n",
        "\tanswer.append(\"S\")\n",
        "else:\n",
        "\tanswer.append(\"N\")\n",
        "\n",
        "b = Counter(TF)\n",
        "value=b.most_common(1)\n",
        "print(value)\n",
        "if value[0][0] == 1:\n",
        "\tanswer.append(\"T\")\n",
        "else:\n",
        "\tanswer.append(\"F\")\n",
        "\n",
        "b = Counter(PJ)\n",
        "value=b.most_common(1)\n",
        "print(value)\n",
        "if value[0][0] == 1:\n",
        "\tanswer.append(\"P\")\n",
        "else:\n",
        "\tanswer.append(\"J\")\n",
        "mbti=\"\".join(answer)\n",
        "print(mbti)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "F8mS1g5D-VwD"
      },
      "id": "F8mS1g5D-VwD"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0bb9de9d-e5a0-4907-aec9-ddc24b6e5821",
      "metadata": {
        "id": "0bb9de9d-e5a0-4907-aec9-ddc24b6e5821"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}