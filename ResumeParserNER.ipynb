{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Visels/ResumeRanker/blob/main/ResumeParserNER.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#RESUME PARSING USING NER"
      ],
      "metadata": {
        "id": "U17HHyQ9L062"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Installing necessary libraries"
      ],
      "metadata": {
        "id": "R8BnzsobL7iy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "09JaIUm0LvX8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e38d1dc0-a6c2-4d5f-9283-7090c94f8bea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: PyMuPDF in /usr/local/lib/python3.10/dist-packages (1.22.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install spacy_transformers\n",
        "!pip install -U spacy\n",
        "!pip install PyMuPDF"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Importing necessary files"
      ],
      "metadata": {
        "id": "xqpBvzj3NkRx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from spacy.tokens import DocBin\n",
        "from tqdm import tqdm\n",
        "import json\n",
        "import pickle\n",
        "import locale"
      ],
      "metadata": {
        "id": "cgjbAPOsQsC6"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spacy.__version__"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "XZjSxJJZJDj-",
        "outputId": "6cb6a035-a941-437d-a7a2-3a8d0908ce5c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'3.5.2'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def getpreferredencoding(do_setlocale = True):\n",
        "    return \"UTF-8\"\n",
        "locale.getpreferredencoding = getpreferredencoding\n",
        "!git clone https://github.com/laxmimerit/CV-Parsing-using-Spacy-3.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2hYtNzcSPuhN",
        "outputId": "83574239-ab83-4f35-f75f-3302b053517e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'CV-Parsing-using-Spacy-3'...\n",
            "remote: Enumerating objects: 82, done.\u001b[K\n",
            "remote: Counting objects: 100% (82/82), done.\u001b[K\n",
            "remote: Compressing objects: 100% (78/78), done.\u001b[K\n",
            "remote: Total 82 (delta 16), reused 5 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (82/82), 5.33 MiB | 5.76 MiB/s, done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# cv_data = pickle.load(open('/content/CV-Parsing-using-Spacy-3/data/training/train_data.json', 'rb'))\n",
        "# cv_data = pickle.load(\"/content/CV-Parsing-using-Spacy-3/data/training/train_data.pkl\")\n",
        "\n",
        "\n",
        "\n",
        "# cv_data = pickle.load(open('/content/CV-Parsing-using-Spacy-3/data/training/train_data.pkl', 'rb'))\n",
        "# cv_data = json.load(open('/content/CV-Parsing-using-Spacy-3/data/training/train_data.json'))\n",
        "cv_data = json.load(open('/content/train_data.json'))"
      ],
      "metadata": {
        "id": "T30Yc6GnQwhe"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "with open('train_data.json', 'r') as f:\n",
        "    training_data = json.load(f)\n",
        "\n",
        "for text,annot in training_data:\n",
        "\n",
        "    for entity in annot['entities']:\n",
        "        start, end, label = entity\n",
        "        # Remove trailing white spaces from the entity text\n",
        "        text = text[start:end].strip()\n",
        "        example['text'] = example['text'][:start] + text + example['text'][end:]\n",
        "\n",
        "with open('training_data_cleaned.json', 'w') as f:\n",
        "    json.dump(training_data, f)"
      ],
      "metadata": {
        "id": "CSmpAzXUadq7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(cv_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M-z3hmk4S6wL",
        "outputId": "873f23cb-6295-4e24-a8bd-f4db73c18c19"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "200"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy init fill-config '/content/CV-Parsing-using-Spacy-3/data/training/base_config.cfg' config.cfg"
      ],
      "metadata": {
        "id": "J-bzTEWaS-lq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8eb38770-048f-4e33-c0d5-c002123d174a"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-05-06 23:21:44.312744: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[38;5;2m✔ Auto-filled config with all values\u001b[0m\n",
            "\u001b[38;5;2m✔ Saved config\u001b[0m\n",
            "config.cfg\n",
            "You can now add your data and train your pipeline:\n",
            "python -m spacy train config.cfg --paths.train ./train.spacy --paths.dev ./dev.spacy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cv_data[2]"
      ],
      "metadata": {
        "id": "ATfMpuKwTmxp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def get_spacy_doc(file,data):\n",
        "#   nlp = spacy.blank('en')\n",
        "#   db = DocBin()\n",
        "\n",
        "#   for text, annot in tqdm(data):\n",
        "#     doc = nlp.make_doc(text)\n",
        "#     annot = annot['entities']\n",
        "\n",
        "#     ents = []\n",
        "#     entity_indices = []\n",
        "\n",
        "#     print(annot)\n",
        "#     for start, end, label in annot:\n",
        "#     # for start, end, label in annot['entities']:\n",
        "#       skip_entity = False\n",
        "\n",
        "#       for idx in range(start, end):\n",
        "#         if idx  in entity_indices:\n",
        "#           skip_entity= False\n",
        "#           break\n",
        "#         if skip_entity == True:\n",
        "#           continue\n",
        "\n",
        "#         entity_indices = entity_indices + list(range(start, end))\n",
        "\n",
        "#         try:\n",
        "#           span =  doc.char_span(start, end, label = label, alignment_mode= 'strict')\n",
        "#         except:\n",
        "#           continue\n",
        "        \n",
        "#         if span is None:\n",
        "#           err_data = str([start, end])+ \"  \" + 'some error' + \"\\n\"\n",
        "#           # err_data = str([start, end]).join(text).encode('utf-8').strip()\n",
        "#           file.write(err_data)\n",
        "\n",
        "#         else:\n",
        "#           ents.append(span)\n",
        "\n",
        "#     try:\n",
        "#       doc.ents = ents\n",
        "#       db.add(doc)\n",
        "\n",
        "#     except:\n",
        "#       pass\n",
        "\n",
        "#   return db"
      ],
      "metadata": {
        "id": "qRlbahW_WQEp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def get_spacy_doc(file,data):\n",
        "\n",
        "\n",
        "#     nlp = spacy.blank('en')\n",
        "#     db = DocBin()\n",
        "\n",
        "\n",
        "\n",
        "#     for text, annot in tqdm(data):\n",
        "#         doc = nlp.make_doc(text)\n",
        "#         annot = annot['entities']\n",
        "\n",
        "#         ents = []\n",
        "#         entity_indices = []\n",
        "\n",
        "        \n",
        "\n",
        "    \n",
        "#         for start, end, label in annot:\n",
        "#           skip_entity = False\n",
        "#           for idx in range(start, end):\n",
        "#             if idx  in entity_indices:\n",
        "#                 skip_entity= False\n",
        "#                 break\n",
        "#         if skip_entity == True:\n",
        "#             continue\n",
        "            \n",
        "#         entity_indices = entity_indices + list(range(start, end))\n",
        "\n",
        "#         try:\n",
        "#             span =  doc.char_span(start, end, label = label, alignment_mode= 'strict')\n",
        "#         except:\n",
        "#             continue\n",
        "\n",
        "        \n",
        "          \n",
        "#         if span is None:\n",
        "#             err_data = str([start, end])+ \"  \" + 'some error' + \"\\n\"\n",
        "#             # err_data = str([start, end]).join(text).encode('utf-8').strip()\n",
        "#             file.write(err_data)\n",
        "#         else:\n",
        "#             ents.append(span)\n",
        "\n",
        "#         try:\n",
        "#             doc.ents = ents\n",
        "#             db.add(doc)\n",
        "\n",
        "#         except:\n",
        "#             pass\n",
        "       \n",
        "\n",
        "#     return db\n",
        "  \n",
        "    \n",
        "#I was using this\n",
        "        "
      ],
      "metadata": {
        "id": "plbW9Mn5tLTw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_spacy_doc(file,data):\n",
        "    nlp = spacy.blank('en')\n",
        "    db = DocBin()\n",
        "\n",
        "\n",
        "    for text, annot in tqdm(data):\n",
        "        doc = nlp.make_doc(text)\n",
        "        annot = annot['entities']\n",
        "        ents = []\n",
        "        entity_indices = []\n",
        "\n",
        "        for start,end, label in annot:\n",
        "            skip_entity = False\n",
        "\n",
        "            for idx in range(start, end):\n",
        "                if idx in entity_indices:\n",
        "                    skip_entity = True\n",
        "                    break\n",
        "\n",
        "            if skip_entity == True:\n",
        "                continue\n",
        "\n",
        "            entity_indices = entity_indices  + list(range(start, end))\n",
        "\n",
        "            try:\n",
        "                span = doc.char_span(start, end, label = label, alignment_mode = 'strict')\n",
        "            except:\n",
        "                continue\n",
        "            \n",
        "            if span is None:\n",
        "                # err_data = str([start, end] + \" \"  + \"\\n\")\n",
        "                err_data = f\"error occured at [{start}, {end}] : some error \\n\"\n",
        "                file.write(err_data)\n",
        "            if span is not None:\n",
        "                ents.append(span)\n",
        "    \n",
        "    \n",
        "        try:\n",
        "            doc.ents = ents\n",
        "            db.add(doc)\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "    return db\n",
        "\n"
      ],
      "metadata": {
        "id": "p77IySqqXiRh"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import re\n",
        "def trim_entity_spans(data: list) -> list:\n",
        "    \"\"\"Removes leading and trailing white spaces from entity spans.\n",
        "\n",
        "    Args:\n",
        "        data (list): The data to be cleaned in spaCy JSON format.\n",
        "\n",
        "    Returns:\n",
        "        list: The cleaned data.\n",
        "    \"\"\"\n",
        "    # invalid_span_tokens = re.compile(r'\\s')\n",
        "    invalid_span_tokens = re.compile(r\"^\\s+|\\s+$\")\n",
        "\n",
        "    cleaned_data = []\n",
        "    for text, annotations in data:\n",
        "        entities = annotations['entities']\n",
        "        valid_entities = []\n",
        "        for start, end, label in entities:\n",
        "            valid_start = start\n",
        "            valid_end = end\n",
        "            while valid_start < len(text) and invalid_span_tokens.match(\n",
        "                    text[valid_start]):\n",
        "                valid_start += 1\n",
        "            while valid_end > 1 and invalid_span_tokens.match(\n",
        "                    text[valid_end - 1]):\n",
        "                valid_end -= 1\n",
        "            valid_entities.append([valid_start, valid_end, label])\n",
        "        cleaned_data.append([text, {'entities': valid_entities}])\n",
        "\n",
        "    return cleaned_data"
      ],
      "metadata": {
        "id": "cZFIM9C0iB13"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def clean_entity_spans(data: list) -> list:\n",
        "  invalid_span_tokens = re.compile(r'\\s')\n",
        "\n",
        "  cleaned_data = []\n",
        "\n",
        "  for content in data:\n",
        "      name = content['documentName']\n",
        "      text = content['document']\n",
        "      userinput = content['user_input']\n",
        "\n",
        "      valid_entities = []\n",
        "\n",
        "      for annotate_content in content['annotation']:\n",
        "          start = annotate_content['start']\n",
        "          end = annotate_content['end']\n",
        "          label = annotate_content['label']\n",
        "          text1 = annotate_content['text']\n",
        "\n",
        "          valid_start = start\n",
        "          valid_end = end\n",
        "\n",
        "          while valid_start < len(text) and invalid_span_tokens.match(\n",
        "                  text[valid_start]):\n",
        "              valid_start += 1\n",
        "          while valid_end > 1 and invalid_span_tokens.match(\n",
        "                  text[valid_end - 1]):\n",
        "              valid_end -= 1\n",
        "          \n",
        "          valid_entities.append({'start': valid_start, 'end': valid_end, 'label': label, 'text': text1, 'propertiesList': [], 'commentsList': []})\n",
        "      cleaned_data.append({'documentName': name, 'document':text, 'annotation': valid_entities, 'user_input': userinput})\n",
        "\n",
        "  return cleaned_data"
      ],
      "metadata": {
        "id": "YTyvWvEOF4dA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# cv_data = trim_entity_spans(cv_data)\n",
        "cv_data"
      ],
      "metadata": {
        "id": "Dq0CBsWCiDQ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "train,test = train_test_split(cv_data, test_size = 0.3)\n",
        "\n",
        "print(\"Training data length:\", len(train))\n",
        "print(\"Test data length:\", len(test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rYHcH6cIcchf",
        "outputId": "f018d7cc-9f0e-4a35-cfb6-50a738a66a10"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training data length: 140\n",
            "Test data length: 60\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file = open('error.txt', 'w')\n",
        "\n",
        "db = get_spacy_doc(file, train)\n",
        "db.to_disk('train_data.spacy')\n",
        "\n",
        "db = get_spacy_doc(file, test)\n",
        "db.to_disk('test_data.spacy')\n",
        "\n",
        "file.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ct-YJ8S4dFky",
        "outputId": "b38e4add-96f5-4ba5-cbb2-66b0294a51b2"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 140/140 [00:01<00:00, 74.81it/s]\n",
            "100%|██████████| 60/60 [00:00<00:00, 64.28it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# viewing the tokens\n",
        "db.tokens.count"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s8pRYa21Pt8Q",
        "outputId": "297f2b18-c52b-44ba-cbb6-63a16bd3c05f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<function list.count(value, /)>"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy train /content/config.cfg  --output ./output --paths.train ./train_data.spacy --paths.dev ./test_data.spacy --gpu-id 0\n",
        "# !python -m spacy train /content/config.cfg  --output ./output --paths.train ./train_data.spacy --paths.dev ./test_data.spacy "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i3pQHqYAyaoC",
        "outputId": "ff600186-2dec-48a4-cb5e-855421917923"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-05-06 23:58:05.080160: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[38;5;4mℹ Saving to output directory: output\u001b[0m\n",
            "\u001b[38;5;4mℹ Using GPU: 0\u001b[0m\n",
            "\u001b[1m\n",
            "=========================== Initializing pipeline ===========================\u001b[0m\n",
            "[2023-05-06 23:58:14,990] [INFO] Set up nlp object from config\n",
            "[2023-05-06 23:58:15,006] [INFO] Pipeline: ['transformer', 'ner']\n",
            "[2023-05-06 23:58:15,009] [INFO] Created vocabulary\n",
            "[2023-05-06 23:58:15,011] [INFO] Finished initializing nlp object\n",
            "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.layer_norm.bias']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "[2023-05-06 23:58:23,863] [INFO] Initialized pipeline components: ['transformer', 'ner']\n",
            "\u001b[38;5;2m✔ Initialized pipeline\u001b[0m\n",
            "\u001b[1m\n",
            "============================= Training pipeline =============================\u001b[0m\n",
            "\u001b[38;5;4mℹ Pipeline: ['transformer', 'ner']\u001b[0m\n",
            "\u001b[38;5;4mℹ Initial learn rate: 0.0\u001b[0m\n",
            "E    #       LOSS TRANS...  LOSS NER  ENTS_F  ENTS_P  ENTS_R  SCORE \n",
            "---  ------  -------------  --------  ------  ------  ------  ------\n",
            "  0       0        2923.28   1593.73    0.22    0.11    4.09    0.00\n",
            "  4     200      265433.02  67837.60   30.48   42.38   23.79    0.30\n",
            "  8     400       39811.87  22874.39   59.41   53.18   67.29    0.59\n",
            " 12     600        5431.69  19059.12   61.46   63.02   59.98    0.61\n",
            " 16     800       40612.53  17910.20   58.86   60.85   57.00    0.59\n",
            " 20    1000        3668.24  16167.96   59.44   64.13   55.39    0.59\n",
            " 24    1200       15654.12  15076.54   60.32   59.45   61.21    0.60\n",
            " 28    1400        3206.04  14072.97   56.89   58.06   55.76    0.57\n",
            " 32    1600        6399.04  13634.35   58.63   56.57   60.84    0.59\n",
            " 36    1800       16076.89  13261.37   58.08   53.37   63.69    0.58\n",
            " 40    2000       19392.17  12827.40   58.22   56.46   60.10    0.58\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy debug data /content/config.cfg --paths.train /content/train_data.spacy\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J-A31u_o7VnW",
        "outputId": "26b02669-d3b8-416b-e176-87e3361e2650"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-05-06 21:09:49.394404: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[1m\n",
            "============================ Data file validation ============================\u001b[0m\n",
            "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.bias']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "\u001b[38;5;2m✔ Pipeline can be initialized with data\u001b[0m\n",
            "/usr/local/lib/python3.10/dist-packages/spacy/training/corpus.py:102: UserWarning: [W090] Could not locate any .spacy files in path '/content/train_data.json'.\n",
            "  warnings.warn(Warnings.W090.format(path=orig_path, format=file_type))\n",
            "\u001b[38;5;2m✔ Corpus is loadable\u001b[0m\n",
            "\u001b[1m\n",
            "=============================== Training stats ===============================\u001b[0m\n",
            "Language: en\n",
            "Training pipeline: transformer, ner\n",
            "140 training docs\n",
            "0 evaluation docs\n",
            "\u001b[38;5;2m✔ No overlap between training and evaluation data\u001b[0m\n",
            "\u001b[38;5;3m⚠ Low number of examples to train a new pipeline (140)\u001b[0m\n",
            "\u001b[1m\n",
            "============================== Vocab & Vectors ==============================\u001b[0m\n",
            "\u001b[38;5;4mℹ 92414 total word(s) in the data (10551 unique)\u001b[0m\n",
            "\u001b[38;5;4mℹ No word vectors present in the package\u001b[0m\n",
            "\u001b[1m\n",
            "========================== Named Entity Recognition ==========================\u001b[0m\n",
            "\u001b[38;5;4mℹ 10 label(s)\u001b[0m\n",
            "0 missing value(s) (tokens with '-' label)\n",
            "\u001b[38;5;3m⚠ Low number of examples for label 'Years of Experience' (31)\u001b[0m\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "\u001b[2K\u001b[38;5;2m✔ Examples without occurrences available for all labels\u001b[0m\n",
            "\u001b[38;5;2m✔ No entities consisting of or starting/ending with whitespace\u001b[0m\n",
            "\u001b[38;5;2m✔ No entities crossing sentence boundaries\u001b[0m\n",
            "\u001b[1m\n",
            "================================== Summary ==================================\u001b[0m\n",
            "\u001b[38;5;2m✔ 6 checks passed\u001b[0m\n",
            "\u001b[38;5;3m⚠ 2 warnings\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Testing the Model"
      ],
      "metadata": {
        "id": "cZ-qyiVv1k8l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load('/content/output/model-last')\n",
        "# nlp = spacy.load('/content/CV-Parsing-using-Spacy-3/nlp_model')\n",
        "\n",
        "doc = nlp('Alice Clark  AI / Machine Learning Delhi, India Email me on Indeed • 20+ years of experience in data handling, design, and development• Data Warehouse: Data analysis, star/snow flake scema data modelling and design specific to data warehousing and business intelligence• Database: Experience in database designing, scalability, back-up and recovery, writing andoptimizing SQL code and Stored Procedures, creating functions, views, triggers and indexes.Cloud platform: Worked on Microsoft Azure cloud services like Document DB, SQL Azure, Stream Analytics, Event hub, Power BI, Web Job, Web App, Power BI, Azure data lake analytics(U-SQL)Willing to relocate anywhereWORK EXPERIENCESoftware EngineerMicrosoft – Bangalore, KarnatakaJanuary 2000 to Present1. Microsoft Rewards Live dashboards:Description: - Microsoft rewards is loyalty program that rewards Users for browsing and shoppingonline. Microsoft Rewards members can earn points when searching with Bing, browsing withMicrosoft Edge and making purchases at the Xbox Store, the Windows Store and the MicrosoftStore. Plus, user can pick up bonus points for taking daily quizzes and tours on the Microsoftrewards website. Rewards live dashboards gives a live picture of usage world-wide and bymarkets like US, Canada, Australia, new user registration count, top/bottom performing reward offers, orders stats and weekly trends of user activities, orders and new user registrations. the PBI tiles gets refreshed in different frequencies starting from 5 seconds to 30 minutes.Technology/Tools used EDUCATION Indian Institute of Technology – Mumbai 2001 SKILLS Machine Learning, Natural Language Processing, and Big Data Handlin')\n",
        "\n",
        "for ent in doc.ents:\n",
        "  print(ent.text, '------>', ent.label_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xv0SRrzJ1oUT",
        "outputId": "c3cc638e-d278-46a2-efdc-779e65b1bdaf"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Microsoft ------> Companies worked at\n",
            "Microsoft ------> Companies worked at\n",
            "Microsoft ------> Companies worked at\n",
            "Microsoft ------> Companies worked at\n",
            "Indian Institute of Technology ------> College Name\n",
            "Machine Learning, Natural Language Processing, and Big Data Handlin ------> Skills\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp('my name is Elvis Kipchumba and I worked at Microsoft I am good at data science and electrical engineering, I can code, program and do many other things')\n",
        "print(doc)\n",
        "for ent in doc.ents:\n",
        "  print(ent.text, '------>', ent.label)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "361yb3xbEzpx",
        "outputId": "98416e24-04d0-4bbe-91cf-94764f5d9e48"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "my name is Elvis Kipchumba and I worked at Microsoft I am good at data science and electrical engineering, I can code, program and do many other things\n",
            "Microsoft ------> 4041747777783172106\n"
          ]
        }
      ]
    }
  ]
}